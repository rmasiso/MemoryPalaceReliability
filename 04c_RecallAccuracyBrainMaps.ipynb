{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-rugby",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "driven-brand",
   "metadata": {},
   "source": [
    "# this notebook is for:\n",
    "\n",
    "1) importing the recall accuracy and confmat scores   \n",
    "2) creating an accuracy brain map via accuracy and confmat forms (although confmat superior here) \n",
    "3a) doing this brainmap for the classification of rooms on room eve nts  \n",
    "3b) doing this brainmap for classification of objects on object events   \n",
    "3c) doing this brainmap for the classification of rooms on object events  (room reinstatement on objects)\n",
    "---> note that 3c means importing recall classificaiton results from 20250524 while 3a and 3b can come from date = 20240401 which is an earlier date and that's fine.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-chile",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suburban-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from _searchlight_util import *\n",
    "import matplotlib.pyplot as plt\n",
    "from _classification_util import Convert_SourceItem_to_PairedItem_Order\n",
    "from _mempal_util import room_obj_assignments, save_obj, load_obj, create_dirs, subject_ids\n",
    "\n",
    "trials = ['GR0', 'GR1', 'GR2', 'GR3','GR4', 'GR5', 'GR6', 'GR7','GR8', 'GR9', 'GR10','FR']\n",
    "roi = 'SL'\n",
    "nSubj =25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hourly-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRecallEvidence(date, roi, shift, win_size, measure_list, betatypes, dirpath = '../PythonData2024/Output/ClassifyRecalls'):\n",
    "    '''\n",
    "    - grabs the whole-brain results of classifying recalls (GRs+FRs)\n",
    "    - returns 'scores' which is a dict with the data in measure_list\n",
    "    \n",
    "    '''\n",
    "\n",
    "    trials = ['GR0', 'GR1', 'GR2', 'GR3','GR4', 'GR5', 'GR6', 'GR7','GR8', 'GR9', 'GR10','FR']\n",
    "\n",
    "    nPerm = 1000\n",
    "    nTrials = len(trials)\n",
    "    nItems   = 23\n",
    "\n",
    "    \n",
    "    scores = {}\n",
    "\n",
    "    for betatype in tqdm_notebook(betatypes):\n",
    "        scores[betatype] = {}\n",
    "\n",
    "        print(\"...BETATYPE: \", betatype)\n",
    "\n",
    "        for measuretype in tqdm_notebook(measure_list):\n",
    "            scores[betatype][measuretype] = {}\n",
    "\n",
    "            print(\"............MEASURE: \", measuretype)\n",
    "\n",
    "            for hem in ['R','L']:\n",
    "                print(\"...............HEM: \",hem)\n",
    "\n",
    "                hem_label = hem\n",
    "                betatype_label=betatype\n",
    "\n",
    "                ###\n",
    "                ### Specific initiation for different array shapes\n",
    "                ###\n",
    "\n",
    "                if 'RIP_window' == measuretype:\n",
    "                    scores[betatype][measuretype][hem] = np.zeros(((len(SLlist[hem]),win_size,nTrials,nSubj))) \n",
    "\n",
    "                if 'evidence_window' == measuretype:\n",
    "                    scores[betatype][measuretype][hem] = np.zeros(((len(SLlist[hem]),win_size,nItems, nTrials,nSubj))) \n",
    "\n",
    "                if 'accuracy' == measuretype:\n",
    "                    scores[betatype][measuretype][hem] = np.zeros(((len(SLlist[hem]),nTrials,nSubj,nPerm+1)))\n",
    "\n",
    "    #             if 'real_minus_prior' == measuretype:\n",
    "    #                 scores[betatype][measuretype][hem] = np.zeros(((len(SLlist[hem]),nTrials,nSubj)))   \n",
    "\n",
    "                if 'evidence' == measuretype:\n",
    "                    scores[betatype][measuretype][hem] = {}\n",
    "\n",
    "                if 'conf_mat' == measuretype: # comes in as (nItems,nItems,nTrials,nSubj,nPerm+1)\n",
    "                    scores[betatype][measuretype][hem] = np.full((len(SLlist[hem]),nItems,nItems,nTrials,nSubj), fill_value=np.nan)\n",
    "#                     scores[betatype][measuretype][hem] = np.full((len(SLlist[hem]),nItems,nItems), fill_value=np.nan)\n",
    "\n",
    "\n",
    "                if 'RIPs' == measuretype:\n",
    "    #                 scores[betatype][measuretype][hem] = np.full((len(SLlist[hem]),nItems,nItems,nTrials,nSubj), fill_value=np.nan)\n",
    "                    scores[betatype][measuretype][hem] = np.full((len(SLlist[hem]),nItems,nTrials,nSubj), fill_value=np.nan)\n",
    "                    \n",
    "\n",
    "\n",
    "                if 'avg_RIPs' == measuretype:\n",
    "                    scores[betatype][measuretype][hem] = np.zeros(((len(SLlist[hem]),2, nItems,nSubj))) # for GR and FR\n",
    "                ###\n",
    "                ### Load the saved .h5 files\n",
    "                ###\n",
    "\n",
    "                for roi_id in tqdm_notebook(range(len(SLlist[hem]))[:]):\n",
    "\n",
    "#                     dirpath = '../PythonData2024/Output/ClassifyRecalls'\n",
    "\n",
    "                    fname = '{}_{}{:03d}_hems{}_betas{}_winsize{}_shift{}_ClassifyRecalls'.format(\n",
    "                    date,\n",
    "                    roi,\n",
    "                    roi_id,\n",
    "                    hem_label,\n",
    "                    betatype_label,\n",
    "                    win_size,\n",
    "                    shift) + '.h5'\n",
    "\n",
    "                    fullpath = os.path.join(dirpath,fname)\n",
    "\n",
    "                    temp = dd.io.load(fullpath,'/{}/{}/{}'.format(betatype,hem,measuretype))\n",
    "\n",
    "                    if measuretype=='conf_mat':\n",
    "#                         print('.......conf_mat shape: ', temp.shape)\n",
    "#                         scores[betatype][measuretype][hem][roi_id] = np.nanmean(np.nanmean(temp[:,:,:,:,0],2),2)\n",
    "                        scores[betatype][measuretype][hem][roi_id,:,:,:,:] = temp[:,:,:,:,0] # permutations are empty, so no need to collect them\n",
    "#                          scores[betatype][measuretype][hem][roi_id,:,:] = temp[:,:,:,:,0].sum(-1).sum(-1) # permutations are empty, so no need to collect them\n",
    "\n",
    "                        \n",
    "                    elif measuretype == \"avg_RIPs\":\n",
    "                        scores[betatype][measuretype][hem][roi_id,0,:,:] = temp['GR']\n",
    "                        scores[betatype][measuretype][hem][roi_id,1,:,:] = temp['FR']\n",
    "                    else:\n",
    "                        scores[betatype][measuretype][hem][roi_id] = temp\n",
    "                        \n",
    "                        \n",
    "    return scores\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-looking",
   "metadata": {},
   "source": [
    "# confmat and accuracy for room recall during room events and object recall during object events (for GRs and FRs)\n",
    "\n",
    "3a and 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-monroe",
   "metadata": {},
   "source": [
    "## confmat\n",
    "\n",
    "run separately, since it takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abroad-extreme",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/ipykernel_launcher.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836737b0ae5847d6ba9440c64487e1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...BETATYPE:  objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/ipykernel_launcher.py:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa17c4d41f54bca9a610c0924045e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............MEASURE:  accuracy\n",
      "...............HEM:  R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/ipykernel_launcher.py:69: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96dcf9e938f146faa6d24607cb82172d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............HEM:  L\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f77298cfe048bc8de158939f915f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...BETATYPE:  rooms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e6c61bb41d468987e4d49e7affcf6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............MEASURE:  accuracy\n",
      "...............HEM:  R\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9e8d80769f4713b39215f8f90dedcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............HEM:  L\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165fc540b93d428bad7232dff2b9767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## \n",
    "## IMPORT -- when rooms classify room events and objects classify object events\n",
    "##\n",
    "\n",
    "#accuracy[betatype][hem]= np.full((nTrials,nSubj,nPerm+1),fill_value=np.nan)\n",
    "# conf_mat[betatype][hem]= np.full((nItems,nItems,nTrials,nSubj,nPerm+1), fill_value=np.nan)\n",
    "# evidence[betatype][hem]= {} #dict of trials and subjects\n",
    "# cp_window[betatype][hem]= np.full((win_size,nTrials,nSubj), fill_value=np.nan) \n",
    "# trial_length[betatype][hem] = np.full((nTrials,nSubj),fill_value=np.nan)\n",
    "\n",
    "dirpath = '../PythonData2024/Output/ClassifyRecalls'\n",
    "\n",
    "\n",
    "date = 20240401; shift = 4; win_size=9 ;\n",
    "\n",
    "\n",
    "# betatypes = ['rooms','objects']\n",
    "betatypes = ['objects', 'rooms'] #['objects']\n",
    "roi = 'SL'\n",
    "measure_list = [\n",
    "#     'conf_mat', #  # only acquire if doing the accurazy brainmap not necessary for the ROCN evidence\n",
    "#     \"avg_RIPs\", \n",
    "#     'evidence',\n",
    "    'accuracy', #\n",
    "#     'RIP_window', # \n",
    "#     'RIPs',\n",
    "#     'conf_mat', # \n",
    "  ]\n",
    "\n",
    "scores = GetRecallEvidence(date, roi, shift, win_size, measure_list, betatypes, dirpath)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-forward",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "recorded-roads",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SAVE CONFMAT since it takes a long time to load up\n",
    "\n",
    "# date = 20240401; shift = 4; win_size=9 ;\n",
    "\n",
    "# fname = '{}_{}_hems{}_betas{}_winsize{}_shift{}_ClassifyRecalls_CONFMAT'.format(\n",
    "# date,\n",
    "# roi,\n",
    "# 'both',\n",
    "# 'both',\n",
    "# win_size,\n",
    "# shift) + '.pkl'\n",
    "\n",
    "# conf_mat_scores = {}\n",
    "# for betatype in tqdm_notebook(betatypes):\n",
    "#     conf_mat_scores[betatype] = {}\n",
    "#     for hem in ['R','L']:\n",
    "#         conf_mat_scores[betatype][hem] = scores[betatype]['conf_mat'][hem].copy()\n",
    "        \n",
    "# save_obj(\"../PythonData2024/Output/confmats/{}\".format(fname),conf_mat_scores)  \n",
    "\n",
    "# print(\"...confmats saved.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-addition",
   "metadata": {},
   "source": [
    "### load confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD AFTER TAKING forever to make confmat array\n",
    "\n",
    "date = 20240401; shift = 4; win_size=9 ;\n",
    "\n",
    "fname = '{}_{}_hems{}_betas{}_winsize{}_shift{}_ClassifyRecalls_CONFMAT'.format(\n",
    "date,\n",
    "roi,\n",
    "'both',\n",
    "'both',\n",
    "win_size,\n",
    "shift) + '.pkl'\n",
    "\n",
    "        \n",
    "conf_mat_scores = load_obj(\"../PythonData2024/Output/confmats/{}\".format(fname))  \n",
    "\n",
    "# print(\"...confmats saved.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "partial-drilling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['objects', 'rooms']),\n",
       " (1483, 23, 23, 12, 25),\n",
       " (1483, 23, 23, 12, 25))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat_scores.keys(), conf_mat_scores['objects']['L'].shape, conf_mat_scores['rooms']['L'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scores --> nPerm+1\n",
    "#ev_prank --> nPerm+1\n",
    "\n",
    "nv = 40962 #verts in fsaverage6 brain\n",
    "# raw_vox = {} # raw values\n",
    "# p_vox = {} # non-parametric p-values\n",
    "# q_vox = {} # q-values from FDR-correction\n",
    "# z_vox = {} # z-values\n",
    "\n",
    "#     'cp_window', # (nSLs, 9,11,25)\n",
    "#     'evidence_window', # (nSLs, 9,23,11,25)\n",
    "#     'accuracy', #(nSLs,11,25,1001)\n",
    "#     'real_minus_prior', #(nSLs,11,25)\n",
    "\n",
    "chance = 1/23\n",
    "\n",
    "trials = ['GR0', 'GR1', 'GR2', 'GR3','GR4', 'GR5', 'GR6', 'GR7','GR8', 'GR9', 'GR10','FR']\n",
    "nPerm = 1000\n",
    "\n",
    "# trial_to_use = 'GR'\n",
    "# trials_to_use\n",
    "measures = {}\n",
    "for trial_to_use in ['GR','FR']:\n",
    "    \n",
    "    ## which type of recall are we going to get accuracy from?\n",
    "    ## choose between guided recalls (GR) or free recalls (FR)\n",
    "    trials_to_use = np.arange(len(trials))[:-1] if trial_to_use == 'GR' else [np.arange(len(trials))[-1]]\n",
    "\n",
    "    measures[trial_to_use] = {}\n",
    "    for betatype in ['objects','rooms']:# ['rooms','objects']:\n",
    "        measures[trial_to_use][betatype] = {}\n",
    "        \n",
    "        for measuretype in ['accuracy','conf_mat']: #measure_list:\n",
    "            measures[trial_to_use][betatype][measuretype] = {}\n",
    "\n",
    "            if 'accuracy' == measuretype:\n",
    "\n",
    "                print(\"....first if: {} {}\".format(betatype, measuretype))\n",
    "\n",
    "                dd_hem = {\"L\":np.nanmean(np.nanmean(scores[betatype][measuretype]['L'][:,trials_to_use,:,:],1),1),\n",
    "                          \"R\": np.nanmean(np.nanmean(scores[betatype][measuretype]['R'][:,trials_to_use,:,:],1),1)} #shape (nSLs,nPerm+1)\n",
    "                dd_vox = SLtoVox(dd_hem, ROIlist,nv,zeronan=False) #\n",
    "\n",
    "                ## z-score\n",
    "                measures[trial_to_use][betatype][measuretype]['raw'] = {}\n",
    "                measures[trial_to_use][betatype][measuretype]['z'] = {}\n",
    "\n",
    "                dd_vox_p = {}\n",
    "                for hem in ['L','R']:\n",
    "                    dd_vox_p[hem] = NonparametricP(dd_vox[hem],sided=1) #accuracy can only be positive so 1-sided\n",
    "                    measures[trial_to_use][betatype][measuretype]['raw'][hem] = dd_vox[hem][:,0]\n",
    "                    measures[trial_to_use][betatype][measuretype]['z'][hem] = nullZ(dd_vox[hem])\n",
    "\n",
    "                measures[trial_to_use][betatype][measuretype]['p'] = dd_vox_p\n",
    "                measures[trial_to_use][betatype][measuretype]['q'] = FDR_p_hem(dd_vox_p)\n",
    "                \n",
    "\n",
    "            ### SHOULD REMOVE THIS, 20230812 DONT NEED IT\n",
    "            if 'RIPs' == measuretype or \"avg_RIPs\" == measuretype:\n",
    "                t_vox = {}\n",
    "                t_p_vox = {}\n",
    "                raw_vox = {}\n",
    "\n",
    "                if 'RIPs'==measuretype:\n",
    "                    dd_hem = {\"L\":np.nanmean(np.nanmean(scores[betatype][measuretype]['L'][:,:,trials_to_use,:],1),1),\n",
    "                          \"R\": np.nanmean(np.nanmean(scores[betatype][measuretype]['R'][:,:,trials_to_use,:],1),1)} #shape (nSLs,nSubj)\n",
    "\n",
    "                elif \"avg_RIPs\" == measuretype:\n",
    "                    idx2use = 0 if trial_to_use=='GR' else 1\n",
    "                    #scores[betatype]['avg_RIPs']['R'].shape #(1483, 2, 23, 25)\n",
    "                    dd_hem = {\"L\": np.nanmean(scores[betatype][measuretype]['L'][:,idx2use,:,:],1),\n",
    "                             \"R\": np.nanmean(scores[betatype][measuretype]['R'][:,idx2use,:,:],1)} #shape (nSLs,nSubj)\n",
    "                \n",
    "                dd_vox = SLtoVox(dd_hem, ROIlist,nv,zeronan=False) #\n",
    "\n",
    "                for hem in ['L','R']:\n",
    "                    t_vox[hem] = np.full((nv),fill_value=np.nan)\n",
    "                    t_p_vox[hem] = np.full((nv),fill_value=np.nan)\n",
    "                    raw_vox[hem] = np.nanmean(dd_vox[hem],1) #avg out subj\n",
    "                    for vi in tqdm_notebook(range(nv)):\n",
    "\n",
    "                        # get t value from 25 subj numbers for every vertex\n",
    "    #                     t, p = stats.ttest_1samp(dd_vox[hem][vi,:],chance,alternative='greater') #one sided t-test \n",
    "                        t, p = stats.ttest_1samp(dd_vox[hem][vi],chance) #only two-sided available\n",
    "                        p = p / 2 # make it 1 sided\n",
    "                        t_vox[hem][vi] = t\n",
    "                        t_p_vox[hem][vi] = p\n",
    "\n",
    "                measures[trial_to_use][betatype][measuretype]['t_q'] = FDR_p_hem(t_p_vox)\n",
    "                measures[trial_to_use][betatype][measuretype]['t'] = t_vox\n",
    "                measures[trial_to_use][betatype][measuretype]['raw'] = raw_vox\n",
    "                \n",
    "            if 'conf_mat' == measuretype: #accuracy based on confmat\n",
    "                \n",
    "                confmat_acc_perm = {}\n",
    "                for hem in ['L','R']:\n",
    "                    confmat_acc_perm[hem] = np.full((len(SLlist[hem]), nPerm+1), fill_value=np.nan)\n",
    "                    confmat = scores[betatype]['conf_mat'][hem][:,:,:,trials_to_use,:].sum(-1).sum(-1) # shape (nSLs, 23,23)\n",
    "\n",
    "                    ## PERMUTATION:\n",
    "                    print('...runing confmat permutation.')\n",
    "                    for sl in tqdm_notebook(range(len(SLlist[hem]))):\n",
    "                        for p in range(nPerm+1):\n",
    "                            if p > 0:\n",
    "                                permconf = confmat[sl,np.random.permutation(np.arange(23)),:].copy()\n",
    "                            else:\n",
    "                                permconf = confmat[sl].copy()\n",
    "\n",
    "                            confmat_acc_perm[hem][sl,p] = np.diag(permconf).sum() / permconf.sum()\n",
    "                            \n",
    "                    \n",
    "                dd_hem = {\"L\":confmat_acc_perm['L'],\"R\": confmat_acc_perm['R']} #shape (nSLs,nPerm+1)\n",
    "                dd_vox = SLtoVox(dd_hem, ROIlist,nv,zeronan=False) #\n",
    "\n",
    "                ## z-score\n",
    "                measures[trial_to_use][betatype][measuretype]['raw'] = {}\n",
    "                measures[trial_to_use][betatype][measuretype]['z'] = {}\n",
    "                \n",
    "                dd_vox_p = {}\n",
    "                for hem in ['L','R']:\n",
    "                    dd_vox_p[hem] = NonparametricP(dd_vox[hem],sided=1) #accuracy can only be positive so 1-sided\n",
    "                    measures[trial_to_use][betatype][measuretype]['raw'][hem] = dd_vox[hem][:,0]\n",
    "                    measures[trial_to_use][betatype][measuretype]['z'][hem] = nullZ(dd_vox[hem])\n",
    "\n",
    "                measures[trial_to_use][betatype][measuretype]['p'] = dd_vox_p\n",
    "                measures[trial_to_use][betatype][measuretype]['q'] = FDR_p_hem(dd_vox_p)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "###\n",
    "# ### SAVE SAVE SAVE\n",
    "# ###\n",
    "# date = 20230814 # after re-running classifyrecalls on objects\n",
    "# date = 20240108 # \n",
    "date = 20240401 # second permutation fix, keep structure intact\n",
    "\n",
    "dirpath = '../PythonData2024/Output/brainmaps/'\n",
    "fname = '{}_ClassifyRecalls_Accuracy_shift{}'.format(date,shift) + '.pkl'\n",
    "fullpath = os.path.join(dirpath,fname)\n",
    "\n",
    "#     data = measures\n",
    "save_obj(fullpath,measures)\n",
    "print('...saving in: ',fullpath)\n",
    "\n",
    "\n",
    "print('complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-production",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-asset",
   "metadata": {},
   "source": [
    "# confmat and accuracy for room recall during object events (for GRs and FRs)\n",
    "\n",
    "3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### IMPORT --- when rooms classify object events [revision update]\n",
    "###\n",
    "\n",
    "dirpath = '../PythonData2024/Output/ClassifyRecalls_RoomsOnObjectEvents/'\n",
    "\n",
    "\n",
    "date = 20250524; shift = 4; win_size=9 ;\n",
    "betatypes    = ['rooms']\n",
    "roi          = 'SL'\n",
    "measure_list = [\n",
    "    'conf_mat', #  # only acquire if doing the accurazy brainmap not necessary for the RRCN evidence\n",
    "#     \"avg_RIPs\", \n",
    "#     'evidence',\n",
    "#     'accuracy', #\n",
    "#     'RIP_window', # \n",
    "#     'RIPs',\n",
    "#     'conf_mat', # \n",
    "  ]\n",
    "\n",
    "scores2 = GetRecallEvidence(date, roi, shift, win_size, measure_list, betatypes, dirpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SAVE CONFMAT since it takes a long time to load up\n",
    "\n",
    "# # date = 20240401; shift = 4; win_size=9 ;\n",
    "\n",
    "# fname = '{}_{}_hems{}_betas{}_winsize{}_shift{}_ClassifyRecalls_RoomsOnObjectEvents_CONFMAT'.format(\n",
    "# date,\n",
    "# roi,\n",
    "# 'both',\n",
    "# 'both',\n",
    "# win_size,\n",
    "# shift) + '.pkl'\n",
    "\n",
    "# conf_mat_scores = {}\n",
    "# for betatype in tqdm_notebook(betatypes):\n",
    "#     conf_mat_scores[betatype] = {}\n",
    "#     for hem in ['R','L']:\n",
    "#         conf_mat_scores[betatype][hem] = scores2[betatype]['conf_mat'][hem].copy()\n",
    "        \n",
    "# save_obj(\"../PythonData2024/Output/confmats/{}\".format(fname),conf_mat_scores)  \n",
    "\n",
    "# print(\"...confmats saved.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "# date = 20240401; shift = 4; win_size=9 ;\n",
    "\n",
    "fname = '{}_{}_hems{}_betas{}_winsize{}_shift{}_ClassifyRecalls_RoomsOnObjectEvents_CONFMAT'.format(\n",
    "date,\n",
    "roi,\n",
    "'both',\n",
    "'both',\n",
    "win_size,\n",
    "shift) + '.pkl'\n",
    "        \n",
    "scores2 = load_obj(\"../PythonData2024/Output/confmats/{}\".format(fname))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### IMPORT --- when rooms classify object events [revision update]\n",
    "###\n",
    "\n",
    "dirpath = '../PythonData2024/Output/ClassifyRecalls_RoomsOnObjectEvents/'\n",
    "\n",
    "\n",
    "date = 20250524; shift = 4; win_size=9 ;\n",
    "betatypes    = ['rooms']\n",
    "roi          = 'SL'\n",
    "measure_list = [\n",
    "#     'conf_mat', #  # only acquire if doing the accurazy brainmap not necessary for the RRCN evidence\n",
    "    \"avg_RIPs\", \n",
    "#     'evidence',\n",
    "    'accuracy', #\n",
    "#     'RIP_window', # \n",
    "#     'RIPs',\n",
    "#     'conf_mat', # \n",
    "  ]\n",
    "\n",
    "scores = GetRecallEvidence(date, roi, shift, win_size, measure_list, betatypes, dirpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scores2 --> nPerm+1\n",
    "#ev_prank --> nPerm+1\n",
    "\n",
    "nv = 40962 #verts in fsaverage6 brain\n",
    "# raw_vox = {} # raw values\n",
    "# p_vox = {} # non-parametric p-values\n",
    "# q_vox = {} # q-values from FDR-correction\n",
    "# z_vox = {} # z-values\n",
    "\n",
    "#     'cp_window', # (nSLs, 9,11,25)\n",
    "#     'evidence_window', # (nSLs, 9,23,11,25)\n",
    "#     'accuracy', #(nSLs,11,25,1001)\n",
    "#     'real_minus_prior', #(nSLs,11,25)\n",
    "\n",
    "chance = 1/23\n",
    "\n",
    "trials = ['GR0', 'GR1', 'GR2', 'GR3','GR4', 'GR5', 'GR6', 'GR7','GR8', 'GR9', 'GR10','FR']\n",
    "nPerm = 1000\n",
    "\n",
    "# trial_to_use = 'GR'\n",
    "# trials_to_use\n",
    "measures = {}\n",
    "for trial_to_use in ['GR','FR']:\n",
    "    \n",
    "    ## which type of recall are we going to get accuracy from?\n",
    "    ## choose between guided recalls (GR) or free recalls (FR)\n",
    "    trials_to_use = np.arange(len(trials))[:-1] if trial_to_use == 'GR' else [np.arange(len(trials))[-1]]\n",
    "\n",
    "    measures[trial_to_use] = {}\n",
    "    for betatype in ['rooms']:# ['rooms','objects']:\n",
    "        measures[trial_to_use][betatype] = {}\n",
    "        \n",
    "        for measuretype in ['conf_mat']: #measure_list:\n",
    "            measures[trial_to_use][betatype][measuretype] = {}\n",
    "\n",
    "            if 'accuracy' == measuretype:\n",
    "\n",
    "                print(\"....first if: {} {}\".format(betatype, measuretype))\n",
    "\n",
    "                dd_hem = {\"L\":np.nanmean(np.nanmean(scores2[betatype][measuretype]['L'][:,trials_to_use,:,:],1),1),\n",
    "                          \"R\": np.nanmean(np.nanmean(scores2[betatype][measuretype]['R'][:,trials_to_use,:,:],1),1)} #shape (nSLs,nPerm+1)\n",
    "                dd_vox = SLtoVox(dd_hem, ROIlist,nv,zeronan=False) #\n",
    "\n",
    "                ## z-score\n",
    "                measures[trial_to_use][betatype][measuretype]['raw'] = {}\n",
    "                measures[trial_to_use][betatype][measuretype]['z'] = {}\n",
    "\n",
    "                dd_vox_p = {}\n",
    "                for hem in ['L','R']:\n",
    "                    dd_vox_p[hem] = NonparametricP(dd_vox[hem],sided=1) #accuracy can only be positive so 1-sided\n",
    "                    measures[trial_to_use][betatype][measuretype]['raw'][hem] = dd_vox[hem][:,0]\n",
    "                    measures[trial_to_use][betatype][measuretype]['z'][hem] = nullZ(dd_vox[hem])\n",
    "\n",
    "                measures[trial_to_use][betatype][measuretype]['p'] = dd_vox_p\n",
    "                measures[trial_to_use][betatype][measuretype]['q'] = FDR_p_hem(dd_vox_p)\n",
    "                \n",
    "\n",
    "            ### SHOULD REMOVE THIS, 20230812 DONT NEED IT\n",
    "            if 'RIPs' == measuretype or \"avg_RIPs\" == measuretype:\n",
    "                t_vox = {}\n",
    "                t_p_vox = {}\n",
    "                raw_vox = {}\n",
    "\n",
    "                if 'RIPs'==measuretype:\n",
    "                    dd_hem = {\"L\":np.nanmean(np.nanmean(scores2[betatype][measuretype]['L'][:,:,trials_to_use,:],1),1),\n",
    "                          \"R\": np.nanmean(np.nanmean(scores2[betatype][measuretype]['R'][:,:,trials_to_use,:],1),1)} #shape (nSLs,nSubj)\n",
    "\n",
    "                elif \"avg_RIPs\" == measuretype:\n",
    "                    idx2use = 0 if trial_to_use=='GR' else 1\n",
    "                    #scores2[betatype]['avg_RIPs']['R'].shape #(1483, 2, 23, 25)\n",
    "                    dd_hem = {\"L\": np.nanmean(scores2[betatype][measuretype]['L'][:,idx2use,:,:],1),\n",
    "                             \"R\": np.nanmean(scores2[betatype][measuretype]['R'][:,idx2use,:,:],1)} #shape (nSLs,nSubj)\n",
    "                \n",
    "                dd_vox = SLtoVox(dd_hem, ROIlist,nv,zeronan=False) #\n",
    "\n",
    "                for hem in ['L','R']:\n",
    "                    t_vox[hem] = np.full((nv),fill_value=np.nan)\n",
    "                    t_p_vox[hem] = np.full((nv),fill_value=np.nan)\n",
    "                    raw_vox[hem] = np.nanmean(dd_vox[hem],1) #avg out subj\n",
    "                    for vi in tqdm_notebook(range(nv)):\n",
    "\n",
    "                        # get t value from 25 subj numbers for every vertex\n",
    "    #                     t, p = stats.ttest_1samp(dd_vox[hem][vi,:],chance,alternative='greater') #one sided t-test \n",
    "                        t, p = stats.ttest_1samp(dd_vox[hem][vi],chance) #only two-sided available\n",
    "                        p = p / 2 # make it 1 sided\n",
    "                        t_vox[hem][vi] = t\n",
    "                        t_p_vox[hem][vi] = p\n",
    "\n",
    "                measures[trial_to_use][betatype][measuretype]['t_q'] = FDR_p_hem(t_p_vox)\n",
    "                measures[trial_to_use][betatype][measuretype]['t'] = t_vox\n",
    "                measures[trial_to_use][betatype][measuretype]['raw'] = raw_vox\n",
    "                \n",
    "            if 'conf_mat' == measuretype: #accuracy based on confmat\n",
    "                \n",
    "                confmat_acc_perm = {}\n",
    "                for hem in ['L','R']:\n",
    "                    confmat_acc_perm[hem] = np.full((len(SLlist[hem]), nPerm+1), fill_value=np.nan)\n",
    "                    confmat = scores2[betatype]['conf_mat'][hem][:,:,:,trials_to_use,:].sum(-1).sum(-1) # shape (nSLs, 23,23)\n",
    "\n",
    "                    ## PERMUTATION:\n",
    "                    print('...runing confmat permutation.')\n",
    "                    for sl in tqdm_notebook(range(len(SLlist[hem]))):\n",
    "                        for p in range(nPerm+1):\n",
    "                            if p > 0:\n",
    "                                permconf = confmat[sl,np.random.permutation(np.arange(23)),:].copy()\n",
    "                            else:\n",
    "                                permconf = confmat[sl].copy()\n",
    "\n",
    "                            confmat_acc_perm[hem][sl,p] = np.diag(permconf).sum() / permconf.sum()\n",
    "                            \n",
    "                    \n",
    "                dd_hem = {\"L\":confmat_acc_perm['L'],\"R\": confmat_acc_perm['R']} #shape (nSLs,nPerm+1)\n",
    "                dd_vox = SLtoVox(dd_hem, ROIlist,nv,zeronan=False) #\n",
    "\n",
    "                ## z-score\n",
    "                measures[trial_to_use][betatype][measuretype]['raw'] = {}\n",
    "                measures[trial_to_use][betatype][measuretype]['z'] = {}\n",
    "                \n",
    "                dd_vox_p = {}\n",
    "                for hem in ['L','R']:\n",
    "                    dd_vox_p[hem] = NonparametricP(dd_vox[hem],sided=1) #accuracy can only be positive so 1-sided\n",
    "                    measures[trial_to_use][betatype][measuretype]['raw'][hem] = dd_vox[hem][:,0]\n",
    "                    measures[trial_to_use][betatype][measuretype]['z'][hem] = nullZ(dd_vox[hem])\n",
    "\n",
    "                measures[trial_to_use][betatype][measuretype]['p'] = dd_vox_p\n",
    "                measures[trial_to_use][betatype][measuretype]['q'] = FDR_p_hem(dd_vox_p)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "###\n",
    "### SAVE SAVE SAVE\n",
    "###\n",
    "# date = 20230814 # after re-running classifyrecalls on objects\n",
    "# date = 20240108 # \n",
    "# date = 20240401 # second permutation fix, keep structure intact\n",
    "\n",
    "dirpath = '../PythonData2024/Output/brainmaps/'\n",
    "fname = '{}_ClassifyRecalls_ConfMatAccuracy_RoomsOnObjectEvents_shift{}'.format(date,shift) + '.pkl'\n",
    "fullpath = os.path.join(dirpath,fname)\n",
    "\n",
    "#     data = measures\n",
    "save_obj(fullpath,measures)\n",
    "print('...saving in: ',fullpath)\n",
    "\n",
    "\n",
    "print('complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-amplifier",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "heard-thriller",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-oracle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
